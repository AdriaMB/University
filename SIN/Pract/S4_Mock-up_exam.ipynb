{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Session 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session, we will be using one of the classification tasks\n",
    "found in OpenML as the basis for a mock-up exam. More precisely, the\n",
    "task\n",
    "[*semeion*](https://www.openml.org/search?type=data&status=any&id=1501)\n",
    "is selected. This is another handwritten digit dataset. Specifically,\n",
    "it is composed by 1593 handwritten digits from around 80 different\n",
    "people. Each digit is represented as a binary image of 16x16 pixels\n",
    "(256 values)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need to run this code if this is the first time you are running this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in ./venv/lib64/python3.13/site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy>=1.22.0 in ./venv/lib64/python3.13/site-packages (from scikit-learn) (2.3.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in ./venv/lib64/python3.13/site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./venv/lib64/python3.13/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./venv/lib64/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting the mock-up exam, you must download the data\n",
    "(**semeion_X.npy** and **semeion_y.npy**) and the logistic regression\n",
    "library (**Logistic_Regression.py**) from poliFormat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell only when running in Google Colab \n",
    "\n",
    "# You need to upload LogisticRegression.py\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "\n",
    "# You need to upload semeion_X.npy \n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "\n",
    "# You need to upload semeion_y.npy\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can find a baseline result achieved with the logistic regression classifier using default parameters with batch size 10, and devoting 80% of the samples to training and 20% to test (with seed random_state$=$23)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error: 12.5%\n"
     ]
    }
   ],
   "source": [
    "from LogisticRegression import LogisticRegressionClassification, LogisticRegressionTraining\n",
    "import warnings; warnings.filterwarnings(\"ignore\"); import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "X = np.load('semeion_X.npy'); y = np.load('semeion_y.npy')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=23)\n",
    "N = len(X_train); M = len(X_test)\n",
    "\n",
    "# Train and classify\n",
    "W = LogisticRegressionTraining(X_train, y_train, bs=10)\n",
    "haty_test = LogisticRegressionClassification(X_test, W)\n",
    "accuracy = np.sum(haty_test==y_test)/M\n",
    "print(f\"Test error: {1.0-accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "Applying the logistic regression classifier with default parameter values and batch size 10, adjust the maximum number of epochs in logarithmic scale to determine an optimal value. Report the classification error rate on the training and test sets. Is overfitting observed? If so, from what epoch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  bs maxEps     eta trainErr testErr\n",
      "---- ------ ------- -------- -------\n",
      "  10      5 1.0e-02    10.3%   14.4%\n",
      "  10     10 1.0e-02     8.0%   12.5%\n",
      "  10     20 1.0e-02     5.6%   10.7%\n",
      "  10     50 1.0e-02     1.8%    9.4%\n",
      "  10    100 1.0e-02     0.5%    8.5%\n",
      "  10    200 1.0e-02     0.1%    9.1%\n",
      "  10    500 1.0e-02     0.1%    9.7%\n",
      "  10   1000 1.0e-02     0.1%    9.7%\n",
      "  10   2000 1.0e-02     0.1%    9.7%\n"
     ]
    }
   ],
   "source": [
    "print(\"  bs maxEps     eta trainErr testErr\")\n",
    "print(\"---- ------ ------- -------- -------\")\n",
    "bs=10; eta=1e-2;\n",
    "#Logarithmic scale: 5,10,20....\n",
    "for maxEpochs in (5,10,20,50,100,200,500,1000,2000):\n",
    "    #We train the X_train samples with the y_train classes\n",
    "    W = LogisticRegressionTraining(X_train, y_train, bs=bs, maxEpochs=maxEpochs, eta=eta)\n",
    "    #\n",
    "    haty_train = LogisticRegressionClassification(X_train, W)\n",
    "    #\n",
    "    acc_train = np.sum(haty_train == y_train) / N\n",
    "    #\n",
    "    haty_test = LogisticRegressionClassification(X_test, W)\n",
    "    #\n",
    "    acc_test = np.sum(haty_test == y_test)/M\n",
    "    print(f\"{bs:4d} {maxEpochs:6d} {eta:.1e} {1.0-acc_train:8.1%} {1.0-acc_test:7.1%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems that with 100 maxEps(iterations) the trainErr and testErr is minimal. When the number of epocs is too low, we don't train the model too long, adn the error is more notorious. However, when we increase the Epocs, the model start learning and becoming better. It arrives a point where we observe that there is overfitting, as the testErr starts increasing. We see overfitting from 200 epocs up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WHY EPOCHS AND NOT ITERATIONS?: divide training data in cells (batches). Every time we process all batches, that is an Epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "Using maximum number of epochs 2000, learning rate (eta) 1e-2 and\n",
    "applying the logistic regression classifier, adjust the batch size in logarithmic\n",
    "scale to determine an optimal value. Report the classification error rate on the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  bs maxEps     eta trainErr testErr\n",
      "---- ------ ------- -------- -------\n",
      "   1   2000 1.0e-02     0.0%   10.0%\n",
      "   2   2000 1.0e-02     0.0%   10.0%\n",
      "   5   2000 1.0e-02     0.0%    9.7%\n",
      "  10   2000 1.0e-02     0.1%    9.7%\n",
      "  20   2000 1.0e-02     0.0%    9.7%\n",
      "  50   2000 1.0e-02     0.0%    9.7%\n",
      " 100   2000 1.0e-02     0.1%    9.7%\n"
     ]
    }
   ],
   "source": [
    "print(\"  bs maxEps     eta trainErr testErr\")\n",
    "print(\"---- ------ ------- -------- -------\")\n",
    "maxEpochs=2000; eta=1e-2;\n",
    "for bs in (1, 2, 5, 10, 20, 50, 100):\n",
    "    W = LogisticRegressionTraining(X_train,y_train, bs=bs, maxEpochs=maxEpochs, eta=eta)\n",
    "    haty_train = LogisticRegressionClassification(X_train,W)\n",
    "    acc_train = np.sum(haty_train==y_train)/N\n",
    "    haty_test = LogisticRegressionClassification(X_test,W)\n",
    "    acc_test = np.sum(haty_test==y_test)/M\n",
    "    print(f\"{bs:4d} {maxEpochs:6d} {eta:.1e} {1.0-acc_train:8.1%} {1.0-acc_test:7.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We explore now the batch size. By doing so many Epochs, the trainingErrors are reduced to nearly 0%, although the execution time is larger than berffore. If our batch size is smaller, the results are worse. From batch size 5 onwards the testErr is constant (no overfitting?). Therefore, we would choose 5\n",
    "\n",
    "THE BATCH SIZE MUST BE SMALLER THAN THE TRAINING DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "Applying the logistic regression classifier with default parameter values and\n",
    "batch size 10, adjust both the maximum number of epochs and the learning rate\n",
    "(eta) to determine the optimal values. Use in both cases a logarithmic\n",
    "scale. Report the classification error rate on the training and test\n",
    "sets. Discuss the results obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  bs maxEps     eta trainErr testErr\n",
      "---- ------ ------- -------- -------\n",
      "  10      5 1.0e-04    38.5%   42.9%\n",
      "  10     10 1.0e-04    35.8%   42.6%\n",
      "  10     20 1.0e-04    30.5%   34.8%\n",
      "  10     50 1.0e-04    21.5%   25.7%\n",
      "  10    100 1.0e-04    17.2%   20.7%\n",
      "  10    200 1.0e-04    14.1%   17.6%\n",
      "  10    500 1.0e-04    10.7%   14.4%\n",
      "  10   1000 1.0e-04     8.0%   11.6%\n",
      "  10   2000 1.0e-04     5.7%   10.7%\n",
      "  10      5 1.0e-03    21.7%   25.7%\n",
      "  10     10 1.0e-03    17.0%   19.7%\n",
      "  10     20 1.0e-03    14.1%   17.9%\n",
      "  10     50 1.0e-03    10.7%   14.4%\n",
      "  10    100 1.0e-03     8.0%   11.6%\n",
      "  10    200 1.0e-03     5.7%   10.7%\n",
      "  10    500 1.0e-03     1.7%    9.4%\n",
      "  10   1000 1.0e-03     0.5%    9.4%\n",
      "  10   2000 1.0e-03     0.2%    9.7%\n",
      "  10      5 1.0e-02    10.3%   14.4%\n",
      "  10     10 1.0e-02     8.0%   12.5%\n",
      "  10     20 1.0e-02     5.6%   10.7%\n",
      "  10     50 1.0e-02     1.8%    9.4%\n",
      "  10    100 1.0e-02     0.5%    8.5%\n",
      "  10    200 1.0e-02     0.1%    9.1%\n",
      "  10    500 1.0e-02     0.1%    9.7%\n",
      "  10   1000 1.0e-02     0.1%    9.7%\n",
      "  10   2000 1.0e-02     0.1%    9.7%\n",
      "  10      5 1.0e-01     2.2%   11.0%\n",
      "  10     10 1.0e-01     0.5%    9.7%\n",
      "  10     20 1.0e-01     0.1%   10.0%\n",
      "  10     50 1.0e-01     0.0%    9.7%\n",
      "  10    100 1.0e-01     0.0%    9.7%\n",
      "  10    200 1.0e-01     0.0%    9.7%\n",
      "  10    500 1.0e-01     0.0%    9.7%\n",
      "  10   1000 1.0e-01     0.0%    9.7%\n",
      "  10   2000 1.0e-01     0.0%    9.7%\n",
      "  10      5 1.0e+00     0.2%   11.0%\n",
      "  10     10 1.0e+00     0.2%   11.0%\n",
      "  10     20 1.0e+00     0.2%   11.0%\n",
      "  10     50 1.0e+00     0.2%   11.0%\n",
      "  10    100 1.0e+00     0.2%   11.0%\n",
      "  10    200 1.0e+00     0.2%   11.0%\n",
      "  10    500 1.0e+00     0.2%   11.0%\n",
      "  10   1000 1.0e+00     0.2%   11.0%\n",
      "  10   2000 1.0e+00     0.2%   11.0%\n",
      "  10      5 1.0e+01     6.7%   13.5%\n",
      "  10     10 1.0e+01     6.7%   13.5%\n",
      "  10     20 1.0e+01     6.7%   13.5%\n",
      "  10     50 1.0e+01     6.7%   13.5%\n",
      "  10    100 1.0e+01     6.7%   13.5%\n",
      "  10    200 1.0e+01     6.7%   13.5%\n",
      "  10    500 1.0e+01     6.7%   13.5%\n",
      "  10   1000 1.0e+01     6.7%   13.5%\n",
      "  10   2000 1.0e+01     6.7%   13.5%\n",
      "  10      5 1.0e+02     8.0%   11.6%\n",
      "  10     10 1.0e+02     8.0%   11.6%\n",
      "  10     20 1.0e+02     8.0%   11.6%\n",
      "  10     50 1.0e+02     8.0%   11.6%\n",
      "  10    100 1.0e+02     8.0%   11.6%\n",
      "  10    200 1.0e+02     8.0%   11.6%\n",
      "  10    500 1.0e+02     8.0%   11.6%\n",
      "  10   1000 1.0e+02     8.0%   11.6%\n",
      "  10   2000 1.0e+02     8.0%   11.6%\n"
     ]
    }
   ],
   "source": [
    "print(\"  bs maxEps     eta trainErr testErr\")\n",
    "print(\"---- ------ ------- -------- -------\")\n",
    "bs=10;\n",
    "\n",
    "for eta in (1e-4, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2):\n",
    "    for maxEpochs in (5, 10, 20, 50, 100, 200, 500, 1000, 2000):\n",
    "        W = LogisticRegressionTraining(X_train,y_train, bs=bs, maxEpochs=maxEpochs, eta=eta)\n",
    "        haty_train = LogisticRegressionClassification(X_train,W)\n",
    "        acc_train = np.sum(haty_train==y_train)/N\n",
    "        haty_test = LogisticRegressionClassification(X_test,W)\n",
    "        acc_test = np.sum(haty_test==y_test)/M\n",
    "        print(f\"{bs:4d} {maxEpochs:6d} {eta:.1e} {1.0-acc_train:8.1%} {1.0-acc_test:7.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better to run in the outer loop the training rate and inside the maxEpochs, as we can \n",
    "observe the speed with which the trainingError decreases. With low learning rates, we must\n",
    "perform many more epochs in order to change and arrive to a better solution.\n",
    "If the learning rate is too big, the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "According to the results you have obtained, could you claim that this task is linearly separable? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To know if some training data is linearly separable, check the treining Error. If the training Error (at some point) goes to 0, then the separation of classes is possible and thus they are linearly separable. Else, then we cannot be sure if they are: maybe with other parameters the classes can be linearly separated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
