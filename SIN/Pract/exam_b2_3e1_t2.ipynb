{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intelligent Systems SIN 2025/26, lab group 3E1, subgroup **2**.\n",
    "# **A3**. Block 2 lab exam (1.25 points).\n",
    "## 17 December 2025. **TIME: 45 min**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student:\n",
    "Adrià Marín Bayarri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in ./venv/lib64/python3.13/site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy>=1.22.0 in ./venv/lib64/python3.13/site-packages (from scikit-learn) (2.3.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in ./venv/lib64/python3.13/site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./venv/lib64/python3.13/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./venv/lib64/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting this exam, you must download the dataset (samples **X.npy** and labels **y.npy**) and the logistic regression library (**Logistic_Regression.py**) from Poliformat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You only need to execute this cell if running in Google Colab \n",
    "\n",
    "# You need to upload LogisticRegression.py\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "# You need to upload X.npy \n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "# You need to upload y.npy\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset description**: This dataset contains **1500** samples with **254** features, for a classification task into **29** classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running the code below, you will load the dataset and partition it into 80% of the samples for training and 20% for testing (with seed random_state$=$23), and you will obtain a baseline test error rate (of 12.3%) from a logistic regression classifier trained using default parameters and batch size 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error: 12.3%\n"
     ]
    }
   ],
   "source": [
    "from LogisticRegression import LogisticRegressionClassification, LogisticRegressionTraining\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "X = np.load('X.npy'); y = np.load('y.npy')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=23)\n",
    "N = len(X_train); M = len(X_test)\n",
    "\n",
    "# Train and classify\n",
    "W = LogisticRegressionTraining(X_train, y_train, bs=10)\n",
    "haty_test = LogisticRegressionClassification(X_test, W)\n",
    "accuracy = np.sum(haty_test==y_test)/M\n",
    "print(f\"Test error: {1.0-accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "Applying the logistic regression classifier with default parameter values (eta 1e-2, batch size 10), explore the maximum number of epochs in logarithmic scale to determine an optimal value.\n",
    "\n",
    "According to the results:\n",
    "\n",
    "1.1) What is the optimal value for the maximum numer of epochs? How do you decide which one to choose?\n",
    "\n",
    "1.2) Is overfitting/overtraining observed? If so, from what epoch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  bs maxEps     eta trainErr testErr\n",
      "---- ------ ------- -------- -------\n",
      "  bs maxEps     eta trainErr testErr\n",
      "---- ------ ------- -------- -------\n",
      "  10      5 1.0e-02     5.2%   12.3%\n",
      "  10     10 1.0e-02     4.0%   12.3%\n",
      "  10     20 1.0e-02     2.0%    9.3%\n",
      "  10     50 1.0e-02     0.7%   11.3%\n",
      "  10    100 1.0e-02     0.2%   12.3%\n",
      "  10    200 1.0e-02     0.0%   12.3%\n",
      "  10    500 1.0e-02     0.0%   12.0%\n",
      "  10   1000 1.0e-02     0.0%   12.0%\n",
      "  10   2000 1.0e-02     0.0%   12.0%\n"
     ]
    }
   ],
   "source": [
    "print(\"  bs maxEps     eta trainErr testErr\")\n",
    "print(\"---- ------ ------- -------- -------\")\n",
    "bs=10; eta=1e-2;\n",
    "for maxEpochs in (5, 10, 20, 50, 100, 200, 500, 1000, 2000):\n",
    "    W = LogisticRegressionTraining(X_train,y_train, bs=bs, maxEpochs=maxEpochs, eta=eta)\n",
    "    haty_train = LogisticRegressionClassification(X_train,W)\n",
    "    acc_train = np.sum(haty_train==y_train)/N\n",
    "    haty_test = LogisticRegressionClassification(X_test,W)\n",
    "    acc_test = np.sum(haty_test==y_test)/M\n",
    "    print(f\"{bs:4d} {maxEpochs:6d} {eta:.1e} {1.0-acc_train:8.1%} {1.0-acc_test:7.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers:**\n",
    "\n",
    "1.1) For the best maxEpochs we will choose the one value that reduces both the trainErr and testErr to the maximum possible, without overfitting the model. In our case, we can observe that the testError decreases until maxEpochs = 50, where it increases again, which could mean that the model is specializing too much in classifying our training data and loosing generalisation. For this reason, I would choose maxEpochs = 20, as it accomplishes a good balance between low trainError and testError.\n",
    "\n",
    "1.2) Yes, there is overfitting from Epoch = 50 forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "Using maximum number of epochs 1000, batch size (bs) 10 and applying the logistic regression classifier, adjust the learning rate (eta) in logarithmic scale to determine an optimal value. Report the classification error rate on the training and test sets.\n",
    "\n",
    "Based on the results, what is the optimal value for the learning rate? Discuss the results obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  bs maxEps     eta trainErr testErr\n",
      "---- ------ ------- -------- -------\n",
      "  10   1000 1.0e-04     3.1%    9.3%\n",
      "  10   1000 1.0e-03     0.2%   12.0%\n",
      "  10   1000 1.0e-02     0.0%   12.0%\n",
      "  10   1000 1.0e-01     2.7%   14.7%\n",
      "  10   1000 1.0e+00     9.8%   22.0%\n",
      "  10   1000 1.0e+01    10.4%   15.0%\n",
      "  10   1000 1.0e+02     8.3%   16.0%\n"
     ]
    }
   ],
   "source": [
    "print(\"  bs maxEps     eta trainErr testErr\")\n",
    "print(\"---- ------ ------- -------- -------\")\n",
    "\n",
    "bs=10; maxEpochs = 1000;\n",
    "for eta in (1e-4, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2):\n",
    "    W = LogisticRegressionTraining(X_train,y_train, bs=bs, maxEpochs=maxEpochs, eta=eta)\n",
    "    haty_train = LogisticRegressionClassification(X_train,W)\n",
    "    acc_train = np.sum(haty_train==y_train)/N\n",
    "    haty_test = LogisticRegressionClassification(X_test,W)\n",
    "    acc_test = np.sum(haty_test==y_test)/M\n",
    "    print(f\"{bs:4d} {maxEpochs:6d} {eta:.1e} {1.0-acc_train:8.1%} {1.0-acc_test:7.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "2.1) The learning rate adjusts the speed at which our model updates the weights to the optimal or correct value for prediction. The higher the learning rate, the greater will be the \"jumps\" it will perform. However, using high values of learning rate can make that a model changes too much, leading to incorrect results. For this reason, we should find a learning rate that correctly converges to one set of weights, decreasing the errors in the prediction. In this case, the learning rate (eta) that best fits to this description might be eta=1.0e-04, as it provides the lowest testError from all the learning rates. Its trainError, although it is not 0%, it is sufficiently low to provide us good results.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "Applying the logistic regression classifier with default parameter values and\n",
    "learning rate (eta) 1e-2, explore at the same time both the maximum number of epochs and the batch size\n",
    "(bs) to determine their optimal values together (tip: use a nested loop, with bs in the outer loop for easier interpretability). Use in both cases a logarithmic\n",
    "scale. Report the classification error rate on the training and test\n",
    "sets.\n",
    "\n",
    "Based on the results, what are the optimal values? Discuss the results obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  bs maxEps     eta trainErr testErr\n",
      "---- ------ ------- -------- -------\n",
      "   1      5 1.0e-02    21.8%   25.7%\n",
      "   1     10 1.0e-02    21.8%   25.7%\n",
      "   1     20 1.0e-02    21.8%   25.7%\n",
      "   1     50 1.0e-02    21.8%   25.7%\n",
      "   1    100 1.0e-02    21.8%   25.7%\n",
      "   1    200 1.0e-02    21.8%   25.7%\n",
      "   1    500 1.0e-02    21.8%   25.7%\n",
      "   1   1000 1.0e-02    21.8%   25.7%\n",
      "   1   2000 1.0e-02    21.8%   25.7%\n",
      "   2      5 1.0e-02    10.7%   20.7%\n",
      "   2     10 1.0e-02     7.4%   16.0%\n",
      "   2     20 1.0e-02     1.5%   13.3%\n",
      "   2     50 1.0e-02     1.5%   13.3%\n",
      "   2    100 1.0e-02     1.5%   13.3%\n",
      "   2    200 1.0e-02     1.5%   13.3%\n",
      "   2    500 1.0e-02     1.5%   13.3%\n",
      "   2   1000 1.0e-02     1.5%   13.3%\n",
      "   2   2000 1.0e-02     1.5%   13.3%\n",
      "   5      5 1.0e-02     5.8%   14.7%\n",
      "   5     10 1.0e-02     3.9%   12.0%\n",
      "   5     20 1.0e-02     1.3%   10.3%\n",
      "   5     50 1.0e-02     0.2%   12.7%\n",
      "   5    100 1.0e-02     0.0%   12.7%\n",
      "   5    200 1.0e-02     0.0%   12.0%\n",
      "   5    500 1.0e-02     0.0%   12.0%\n",
      "   5   1000 1.0e-02     0.0%   12.0%\n",
      "   5   2000 1.0e-02     0.0%   12.0%\n",
      "  10      5 1.0e-02     5.2%   12.3%\n",
      "  10     10 1.0e-02     4.0%   12.3%\n",
      "  10     20 1.0e-02     2.0%    9.3%\n",
      "  10     50 1.0e-02     0.7%   11.3%\n",
      "  10    100 1.0e-02     0.2%   12.3%\n",
      "  10    200 1.0e-02     0.0%   12.3%\n",
      "  10    500 1.0e-02     0.0%   12.0%\n",
      "  10   1000 1.0e-02     0.0%   12.0%\n",
      "  10   2000 1.0e-02     0.0%   12.0%\n",
      "  20      5 1.0e-02     5.8%   10.3%\n",
      "  20     10 1.0e-02     5.0%    9.7%\n",
      "  20     20 1.0e-02     2.9%    9.3%\n",
      "  20     50 1.0e-02     1.3%   10.0%\n",
      "  20    100 1.0e-02     0.7%   11.3%\n",
      "  20    200 1.0e-02     0.2%   12.3%\n",
      "  20    500 1.0e-02     0.0%   12.3%\n",
      "  20   1000 1.0e-02     0.0%   12.0%\n",
      "  20   2000 1.0e-02     0.0%   12.0%\n",
      "  50      5 1.0e-02     6.4%   10.0%\n",
      "  50     10 1.0e-02     6.1%   10.3%\n",
      "  50     20 1.0e-02     4.5%   10.0%\n",
      "  50     50 1.0e-02     3.1%    9.7%\n",
      "  50    100 1.0e-02     1.8%   10.3%\n",
      "  50    200 1.0e-02     0.9%   11.0%\n",
      "  50    500 1.0e-02     0.2%   12.0%\n",
      "  50   1000 1.0e-02     0.0%   12.0%\n",
      "  50   2000 1.0e-02     0.0%   12.0%\n",
      " 100      5 1.0e-02     7.3%    9.7%\n",
      " 100     10 1.0e-02     6.7%    9.7%\n",
      " 100     20 1.0e-02     5.8%    9.7%\n",
      " 100     50 1.0e-02     3.8%    9.7%\n",
      " 100    100 1.0e-02     3.1%    9.7%\n",
      " 100    200 1.0e-02     1.8%   10.0%\n",
      " 100    500 1.0e-02     0.7%   11.0%\n",
      " 100   1000 1.0e-02     0.2%   12.0%\n",
      " 100   2000 1.0e-02     0.0%   12.3%\n"
     ]
    }
   ],
   "source": [
    "print(\"  bs maxEps     eta trainErr testErr\")\n",
    "print(\"---- ------ ------- -------- -------\")\n",
    "eta=1e-2;\n",
    "\n",
    "for bs in (1, 2, 5, 10, 20, 50, 100):\n",
    "    for maxEpochs in (5, 10, 20, 50, 100, 200, 500, 1000, 2000):\n",
    "        W = LogisticRegressionTraining(X_train,y_train, bs=bs, maxEpochs=maxEpochs, eta=eta)\n",
    "        haty_train = LogisticRegressionClassification(X_train,W)\n",
    "        acc_train = np.sum(haty_train==y_train)/N\n",
    "        haty_test = LogisticRegressionClassification(X_test,W)\n",
    "        acc_test = np.sum(haty_test==y_test)/M\n",
    "        print(f\"{bs:4d} {maxEpochs:6d} {eta:.1e} {1.0-acc_train:8.1%} {1.0-acc_test:7.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "There are multiple combinations that offer good results for the same learning rate: \n",
    "    -bs = 10, maxEpochs = 20, trainErr = 2.0%, testErr: 9.3%\n",
    "    -bs = 20, maxEpochs = 50, trainErr = 1.3%, testErr: 10%\n",
    "    -bs = 20, maxEpochs = 20, trainErr = 2.9%, testErr: 9.3%\n",
    "Nevertheless, the option I would choose as the most optimal would be bs=10 and maxEpochs=20, as it is the combination that accomplishes the best  testError(9.3%) without sacrificing the trainingError(2.0%), obtaining a good balance between correct predictions and generalisation(no overfitting of the model). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "According to the results you have obtained, can you claim whether this task is linearly separable? Which result(s) can you look at for that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "Yes, we can state that the classes are linearly separable, as we can observe in some combinations of bs and maxEpochs a trainingError = 0.0%. Some examples of this are combinations:\n",
    "    -bs = 50, maxEpochs = 1000\n",
    "    -bs = 100, maxEpochs = 2000\n",
    "    -bs = 5, maxEpochs = 100\n",
    "\n",
    "If there wasn't any combination achieving this 0% in the trainingError, we wouldn't be able to determine if there is linear separation between classes, although it would still be possible"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
