{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 2\n",
    "\n",
    "<div style=\"text-align: justify\">\n",
    "In this second session we will apply the Perceptron algorithm to some classification tasks. A simple implementation of the Perceptron algorithm and its application is provided. The final purpose of this session is to apply the Perceptron algorithm to MyDigits dataset to train a matrix of weights that will be used in an application for Handwritten Digit Classification.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need to run this code if this is the first time you are running this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn scikit-learn pandas pillow gradio matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import warnings; warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perceptron Classification:** classification of samples provided a weight matrix. Samples need to be prefixed by 1\n",
    "\n",
    "$$c(\\boldsymbol{x}) = \\operatorname*{argmax}_{c} g_{c}(\\boldsymbol{x})\\text{, with }g_{c}(\\boldsymbol{x})=\\boldsymbol{w}_{c}^{t}\\boldsymbol{x}\\text{ for all }c$$\n",
    "\n",
    "where $\\boldsymbol{x} = (1, x_1, ..., x_D)^t$, $\\mathbf{W} = (\\boldsymbol{w}_1, \\boldsymbol{w}_2, ...,  \\boldsymbol{w}_C)$ and $\\boldsymbol{w}_c = (w_{c0}, w_{c1}, ..., w_{cD})^t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PerceptronClassification(X, W):\n",
    "  Xh = np.hstack([np.ones((len(X), 1)), X])\n",
    "  return np.argmax(Xh @ W, axis=1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PerceptronTraining:** $\\;$ Perceptron learns a matrix of weights $\\mathbf{W}^*$ that minimizes the number of training errors (with margin $b$)\n",
    "$$\\mathbf{W}^*=\\operatorname*{argmin}_{\\mathbf{W}=(\\boldsymbol{w}_1,\\dotsc,\\boldsymbol{w}_C)}\\sum_n\\;\\mathbb{ I}\\biggl(\\max_{c\\neq y_n}\\;\\boldsymbol{w}_c^t\\boldsymbol{x}_n+b \\;>\\; \\boldsymbol{w}_{y_n}^t\\boldsymbol{ x}_n\\biggr)$$\n",
    "\n",
    "It returns weights in homogeneous notation, $\\mathbf{W}\\in\\mathbb{R}^{(1+D)\\times C};\\;$  together with the number of errors and iterations executed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Input:** $\\;$ data $\\;\\mathcal{D}=\\{(\\boldsymbol{x}_n,y_n)\\}\\quad$ weights $\\;\\mathbf{W}=\\{\\boldsymbol{w}_c\\}\\quad$ learning rate $\\;\\alpha\\in\\mathbb{R}^{>0}\\quad$ margin $\\;b\\in\\mathbb{R}^{\\geq 0}$ <br>\n",
    "> **Output:** $\\;$ optimized weights $\\;\\mathbf{W}^*=\\{\\boldsymbol{w}_c\\}^*$ <br>\n",
    "> `repeat` <br>\n",
    ">> `for all` $\\;$ training sample $\\,\\boldsymbol{x}_n$ <br>\n",
    ">>> *err* = `False` <br>\n",
    ">>> `for all` $\\;$ class $\\,c\\neq y_n$ <br>\n",
    ">>>> `if` $\\;\\boldsymbol{w}_c^t\\boldsymbol{x}_n+b>\\boldsymbol{w}_{y_n}^t\\boldsymbol{x}_n:\\quad\\boldsymbol{w}_c=\\boldsymbol{w}_c-\\alpha\\boldsymbol{x}_n;\\quad$ *err* = `True` <br>\n",
    ">>>\n",
    ">>> `if` $\\;$ *err*: $\\quad \\boldsymbol{w}_{y_n}=\\boldsymbol{w}_{y_n}+\\alpha\\boldsymbol{x}_n$\n",
    ">\n",
    "> `until` $\\;$ no training sample is misclassified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PerceptronTraining(X, y, b=0.1, a=1.0, K=200):\n",
    "    N, D = X.shape; Y = np.unique(y); C = Y.size; W = np.zeros((1+D, C))\n",
    "    for k in range(1, K+1):  # for K iterations\n",
    "        E = 0\n",
    "        for n in range(N):  # for every training sample\n",
    "            xn = np.array([1, *X[n, :]])\n",
    "            cn = np.squeeze(np.where(Y==y[n]))  # Mapping to class labels from 0 to C-1 (for algorithmic simplicity)\n",
    "            gn = W[:,cn].T @ xn; err = False\n",
    "            for c in np.arange(C):  # for every class \n",
    "                if c != cn and W[:,c].T @ xn + b >= gn:\n",
    "                    W[:, c] = W[:, c] - a*xn; err = True\n",
    "            if err:\n",
    "                W[:, cn] = W[:, cn] + a*xn; E = E + 1\n",
    "        if E == 0:\n",
    "            break\n",
    "    return W, E, k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron applied to the Iris dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the dataset:** $\\;$ we also check that the data matrix and labels have the right number of rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4) (150, 1) \n",
      " [[5.1015625  3.5        1.40039062 0.19995117 0.        ]\n",
      " [4.8984375  3.         1.40039062 0.19995117 0.        ]\n",
      " [4.69921875 3.19921875 1.29980469 0.19995117 0.        ]\n",
      " [4.6015625  3.09960938 1.5        0.19995117 0.        ]\n",
      " [5.         3.59960938 1.40039062 0.19995117 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris(); X = iris.data.astype(np.float16)\n",
    "y = iris.target.astype(np.uint).reshape(-1, 1)\n",
    "print(X.shape, y.shape, \"\\n\", np.hstack([X, y])[:5, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset partition:** $\\;$ We create a split of the Iris dataset with $20\\%$ of data for test and the rest for training, previously shuffling the data according to a given seed provided by a random number generator. Here, as in all code that includes randomness (which requires generating random numbers), it is convenient to fix said seed to be able to reproduce experiments with accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4) (30, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=23)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning a (linear) classifier with Perceptron:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations executed:  200\n",
      "Number of training errors:  2\n",
      "Weight vectors of the classes (in columns and with homogeneous notation):\n",
      " [[  10.           85.         -142.        ]\n",
      " [ -49.421875    -68.19140625 -176.47265625]\n",
      " [  50.171875     -1.72460938 -181.06445312]\n",
      " [-189.91210938  -87.70507812   68.69726562]\n",
      " [ -86.40258789 -137.78149414  157.88415527]]\n"
     ]
    }
   ],
   "source": [
    "W, E, k = PerceptronTraining(X_train, y_train)\n",
    "print(\"Number of iterations executed: \", k)\n",
    "print(\"Number of training errors: \", E)\n",
    "print(\"Weight vectors of the classes (in columns and with homogeneous notation):\\n\", W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculation of test error rate:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate on test: 16.7%\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = PerceptronClassification(X_test,W)\n",
    "err_test = np.count_nonzero(y_test_pred != y_test) / len(X_test)\n",
    "print(f\"Error rate on test: {err_test:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adjusting maximum number of iterations:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  alpha       b      K TrErr  TeErr\n",
      "------- ------- ------ ------ ------\n",
      "1.0e+00 1.0e-01      1  49.2%  33.3%\n",
      "1.0e+00 1.0e-01      2  31.7%  50.0%\n",
      "1.0e+00 1.0e-01      5  14.2%  73.3%\n",
      "1.0e+00 1.0e-01     10  12.5%  56.7%\n",
      "1.0e+00 1.0e-01     20  14.2%  26.7%\n",
      "1.0e+00 1.0e-01     50   8.3%  16.7%\n",
      "1.0e+00 1.0e-01    100   9.2%  26.7%\n",
      "1.0e+00 1.0e-01    200   1.7%  16.7%\n",
      "1.0e+00 1.0e-01    500   2.5%   3.3%\n",
      "1.0e+00 1.0e-01   1000   2.5%  13.3%\n",
      "1.0e+00 1.0e-01   2000   5.0%   3.3%\n",
      "1.0e+00 1.0e-01   5000   1.7%   6.7%\n"
     ]
    }
   ],
   "source": [
    "print(f\"  alpha       b      K TrErr  TeErr\")\n",
    "print(f\"------- ------- ------ ------ ------\")\n",
    "b = 0.1; a = 1.0\n",
    "for K in (1, 2, 5, 10, 20, 50, 100, 200, 500, 1000, 2000, 5000):\n",
    "    W, E, k = PerceptronTraining(X_train, y_train, b=b, a=a, K=K)\n",
    "    y_test_pred = PerceptronClassification(X_test,W)\n",
    "    err_test = np.count_nonzero(y_test_pred != y_test) / len(X_test)\n",
    "    print(f\"{a:.1e} {b:.1e} {k:6d} {E/len(X_train):6.1%} {err_test:6.1%}\")\n",
    "\n",
    "    # We search for that number of iterations K such that the TeErr is minimized, but our model does not overfit the training data (turns to be very good at predicting training data and\n",
    "    # very bad at predicting test data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adjusting the learning rate (alpha):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  alpha       b      K TrErr  TeErr\n",
      "------- ------- ------ ------ ------\n",
      "1.0e-03 1.0e-01    500   8.3%   3.3%\n",
      "1.0e-02 1.0e-01    500   2.5%   3.3%\n",
      "1.0e-01 1.0e-01    500   4.2%  16.7%\n",
      "1.0e+00 1.0e-01    500   2.5%   3.3%\n",
      "1.0e+01 1.0e-01    500   4.2%  16.7%\n",
      "1.0e+02 1.0e-01    500   4.2%  16.7%\n",
      "1.0e+03 1.0e-01    500   0.8%  16.7%\n"
     ]
    }
   ],
   "source": [
    "print(f\"  alpha       b      K TrErr  TeErr\")\n",
    "print(f\"------- ------- ------ ------ ------\")\n",
    "b = 0.1; K = 500\n",
    "for a in (1e-3, 1e-2, 1e-1, 1e-0, 1e1, 1e2, 1e3):\n",
    "    W, E, k = PerceptronTraining(X_train, y_train, b=b, a=a, K=K)\n",
    "    y_test_pred = PerceptronClassification(X_test,W)\n",
    "    err_test = np.count_nonzero(y_test_pred != y_test) / len(X_test)\n",
    "    print(f\"{a:.1e} {b:.1e} {k:6d} {E/len(X_train):6.1%} {err_test:6.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adjusting the margin (b):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  alpha       b      K TrErr  TeErr\n",
      "------- ------- ------ ------ ------\n",
      "1.0e+00 0.0e+00    500   0.8%  16.7%\n",
      "1.0e+00 1.0e-02    500   4.2%  16.7%\n",
      "1.0e+00 1.0e-01    500   2.5%   3.3%\n",
      "1.0e+00 1.0e+00    500   4.2%  16.7%\n",
      "1.0e+00 1.0e+01    500   2.5%   3.3%\n",
      "1.0e+00 1.0e+02    500   8.3%   3.3%\n"
     ]
    }
   ],
   "source": [
    "print(f\"  alpha       b      K TrErr  TeErr\")\n",
    "print(f\"------- ------- ------ ------ ------\")\n",
    "a = 1.0; K = 500\n",
    "for b in (.0, .01, .1, 1, 10, 100):\n",
    "    W, E, k = PerceptronTraining(X_train, y_train, b=b, a=a, K=K)\n",
    "    y_test_pred = PerceptronClassification(X_test,W)\n",
    "    err_test = np.count_nonzero(y_test_pred != y_test) / len(X_test)\n",
    "    print(f\"{a:.1e} {b:.1e} {k:6d} {E/len(X_train):6.1%} {err_test:6.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation of results:** $\\;$ the training data does not appear to be linearly separable; it is not clear that a margin greater than zero can improve results, especially since we only have $30$ test samples; with a margin $b=0.1$ we have already seen that an error (in test) of $3.3\\%$ is obtained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron applied to the Digits dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the dataset:** $\\;$ we also check that the data matrix and labels have the right number of rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64) (1797, 1) \n",
      " [[0.     0.     0.3125 0.8125 0.5625 0.0625 0.     0.     0.     0.\n",
      "  0.8125 0.9375 0.625  0.9375 0.3125 0.     0.     0.1875 0.9375 0.125\n",
      "  0.     0.6875 0.5    0.     0.     0.25   0.75   0.     0.     0.5\n",
      "  0.5    0.     0.     0.3125 0.5    0.     0.     0.5625 0.5    0.\n",
      "  0.     0.25   0.6875 0.     0.0625 0.75   0.4375 0.     0.     0.125\n",
      "  0.875  0.3125 0.625  0.75   0.     0.     0.     0.     0.375  0.8125\n",
      "  0.625  0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.75   0.8125 0.3125 0.     0.     0.     0.\n",
      "  0.     0.6875 1.     0.5625 0.     0.     0.     0.     0.1875 0.9375\n",
      "  1.     0.375  0.     0.     0.     0.4375 0.9375 1.     1.     0.125\n",
      "  0.     0.     0.     0.     0.0625 1.     1.     0.1875 0.     0.\n",
      "  0.     0.     0.0625 1.     1.     0.375  0.     0.     0.     0.\n",
      "  0.0625 1.     1.     0.375  0.     0.     0.     0.     0.     0.6875\n",
      "  1.     0.625  0.     0.     1.    ]\n",
      " [0.     0.     0.     0.25   0.9375 0.75   0.     0.     0.     0.\n",
      "  0.1875 1.     0.9375 0.875  0.     0.     0.     0.     0.5    0.8125\n",
      "  0.5    1.     0.     0.     0.     0.     0.0625 0.375  0.9375 0.6875\n",
      "  0.     0.     0.     0.0625 0.5    0.8125 0.9375 0.0625 0.     0.\n",
      "  0.     0.5625 1.     1.     0.3125 0.     0.     0.     0.     0.1875\n",
      "  0.8125 1.     1.     0.6875 0.3125 0.     0.     0.     0.     0.1875\n",
      "  0.6875 1.     0.5625 0.     2.    ]\n",
      " [0.     0.     0.4375 0.9375 0.8125 0.0625 0.     0.     0.     0.5\n",
      "  0.8125 0.375  0.9375 0.25   0.     0.     0.     0.125  0.0625 0.8125\n",
      "  0.8125 0.     0.     0.     0.     0.     0.125  0.9375 0.6875 0.0625\n",
      "  0.     0.     0.     0.     0.     0.0625 0.75   0.75   0.0625 0.\n",
      "  0.     0.     0.     0.     0.0625 0.625  0.5    0.     0.     0.\n",
      "  0.5    0.25   0.3125 0.875  0.5625 0.     0.     0.     0.4375 0.8125\n",
      "  0.8125 0.5625 0.     0.     3.    ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits(); X = digits.images.astype(np.float16).reshape(-1, 8*8); X/=np.max(X)\n",
    "y = digits.target.astype(np.uint).reshape(-1, 1)\n",
    "print(X.shape, y.shape, \"\\n\", np.hstack([X, y])[:4, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset partition:** $\\;$ We create a split of the Iris dataset with $20\\%$ of data for test and the rest for training, previously shuffling the data according to a given seed provided by a random number generator. Here, as in all code that includes randomness (which requires generating random numbers), it is convenient to fix said seed to be able to reproduce experiments with accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1437, 64) (360, 64)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=23)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adjusting maximum number of iterations:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  alpha       b      K TrErr  TeErr\n",
      "------- ------- ------ ------ ------\n",
      "1.0e+00 1.0e-01      1  25.1%  14.7%\n",
      "1.0e+00 1.0e-01      2  13.2%   8.1%\n",
      "1.0e+00 1.0e-01      5   8.4%   8.1%\n",
      "1.0e+00 1.0e-01     10   5.6%   7.5%\n",
      "1.0e+00 1.0e-01     20   2.9%   6.7%\n",
      "1.0e+00 1.0e-01     50   1.9%   5.8%\n",
      "1.0e+00 1.0e-01    100   0.8%   4.7%\n",
      "1.0e+00 1.0e-01    111   0.0%   4.4%\n"
     ]
    }
   ],
   "source": [
    "print(f\"  alpha       b      K TrErr  TeErr\")\n",
    "print(f\"------- ------- ------ ------ ------\")\n",
    "b = 0.1; a = 1.0\n",
    "for K in (1, 2, 5, 10, 20, 50, 100, 200):\n",
    "    W, E, k = PerceptronTraining(X_train, y_train, b=b, a=a, K=K)\n",
    "    y_test_pred = PerceptronClassification(X_test,W)\n",
    "    err_test = np.count_nonzero(y_test_pred != y_test) / len(X_test)\n",
    "    print(f\"{a:.1e} {b:.1e} {k:6d} {E/len(X_train):6.1%} {err_test:6.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adjusting the learning rate (alpha):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  alpha       b      K TrErr  TeErr\n",
      "------- ------- ------ ------ ------\n",
      "1.0e-03 1.0e-01    742   0.0%   4.2%\n",
      "1.0e-02 1.0e-01    205   0.0%   5.0%\n",
      "1.0e-01 1.0e-01    113   0.0%   5.3%\n",
      "1.0e+00 1.0e-01    111   0.0%   4.4%\n",
      "1.0e+01 1.0e-01    130   0.0%   3.6%\n",
      "1.0e+02 1.0e-01    112   0.0%   4.2%\n",
      "1.0e+03 1.0e-01    112   0.0%   4.2%\n"
     ]
    }
   ],
   "source": [
    "print(f\"  alpha       b      K TrErr  TeErr\")\n",
    "print(f\"------- ------- ------ ------ ------\")\n",
    "b = 0.1; K = 1000\n",
    "for a in (1e-3, 1e-2, 1e-1, 1e-0, 1e1, 1e2, 1e3):\n",
    "    W, E, k = PerceptronTraining(X_train, y_train, b=b, a=a, K=K)\n",
    "    y_test_pred = PerceptronClassification(X_test,W)\n",
    "    err_test = np.count_nonzero(y_test_pred != y_test) / len(X_test)\n",
    "    print(f\"{a:.1e} {b:.1e} {k:6d} {E/len(X_train):6.1%} {err_test:6.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adjusting the margin (b):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  alpha       b      K TrErr  TeErr\n",
      "------- ------- ------ ------ ------\n",
      "1.0e+01 0.0e+00    112   0.0%   4.2%\n",
      "1.0e+01 1.0e-02    112   0.0%   4.2%\n",
      "1.0e+01 1.0e-01    130   0.0%   3.6%\n",
      "1.0e+01 1.0e+00    111   0.0%   4.4%\n",
      "1.0e+01 1.0e+01    113   0.0%   5.3%\n",
      "1.0e+01 1.0e+02    187   0.0%   5.0%\n"
     ]
    }
   ],
   "source": [
    "print(f\"  alpha       b      K TrErr  TeErr\")\n",
    "print(f\"------- ------- ------ ------ ------\")\n",
    "a = 1e1; K = 1000\n",
    "for b in (.0, .01, .1, 1, 10, 100):\n",
    "    W, E, k = PerceptronTraining(X_train, y_train, b=b, a=a, K=K)\n",
    "    y_test_pred = PerceptronClassification(X_test,W)\n",
    "    err_test = np.count_nonzero(y_test_pred != y_test) / len(X_test)\n",
    "    print(f\"{a:.1e} {b:.1e} {k:6d} {E/len(X_train):6.1%} {err_test:6.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation of results:** $\\;$ the training data is linearly separable with training error equal to zero. In this case, it seems that small margins provide similar results on the test set with a lowest value of $3.6\\%$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron applied to MyDigits dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell only when running in Google Colab \n",
    "# You need to upload your images and labels files\n",
    "#from google.colab import files\n",
    "#uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "with open('images.npy', 'rb') as fd:\n",
    "    X = np.load(fd)\n",
    "\n",
    "with open('labels.npy', 'rb') as fd:\n",
    "    y = np.load(fd).astype(int).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset partition:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 64) (20, 64)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=23)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adjusting maximum number of iterations:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  alpha       b      K TrErr  TeErr\n",
      "------- ------- ------ ------ ------\n",
      "1.0e+00 1.0e-01      1  66.2%  40.0%\n",
      "1.0e+00 1.0e-01      2  30.0%  20.0%\n",
      "1.0e+00 1.0e-01      5  13.8%  10.0%\n",
      "1.0e+00 1.0e-01     10   2.5%  15.0%\n",
      "1.0e+00 1.0e-01     11   0.0%  15.0%\n",
      "1.0e+00 1.0e-01     11   0.0%  15.0%\n",
      "1.0e+00 1.0e-01     11   0.0%  15.0%\n",
      "1.0e+00 1.0e-01     11   0.0%  15.0%\n"
     ]
    }
   ],
   "source": [
    "print(f\"  alpha       b      K TrErr  TeErr\")\n",
    "print(f\"------- ------- ------ ------ ------\")\n",
    "b = 0.1; a = 1.0\n",
    "for K in (1, 2, 5, 10, 20, 50, 100, 200):\n",
    "    W, E, k = PerceptronTraining(X_train, y_train, b=b, a=a, K=K)\n",
    "    y_test_pred = PerceptronClassification(X_test,W)\n",
    "    err_test = np.count_nonzero(y_test_pred != y_test) / len(X_test)\n",
    "    print(f\"{a:.1e} {b:.1e} {k:6d} {E/len(X_train):6.1%} {err_test:6.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adjusting the learning rate (alpha):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  alpha       a      K TrErr  TeErr\n",
      "------- ------- ------ ------ ------\n",
      "1.0e-03 1.0e-01     10  98.8%  10.0%\n",
      "1.0e-02 1.0e-01     10  28.7%   5.0%\n",
      "1.0e-01 1.0e-01     10   0.0%   5.0%\n",
      "1.0e+00 1.0e-01     10   2.5%  15.0%\n",
      "1.0e+01 1.0e-01     10   0.0%  25.0%\n",
      "1.0e+02 1.0e-01     10   0.0%  25.0%\n",
      "1.0e+03 1.0e-01     10   0.0%  25.0%\n"
     ]
    }
   ],
   "source": [
    "print(f\"  alpha       a      K TrErr  TeErr\")\n",
    "print(f\"------- ------- ------ ------ ------\")\n",
    "b = 0.1; K = 10\n",
    "for a in (1e-3, 1e-2, 1e-1, 1e-0, 1e1, 1e2, 1e3):\n",
    "    W, E, k = PerceptronTraining(X_train, y_train, b=b, a=a, K=K)\n",
    "    y_test_pred = PerceptronClassification(X_test,W)\n",
    "    err_test = np.count_nonzero(y_test_pred != y_test) / len(X_test)\n",
    "    print(f\"{a:.1e} {b:.1e} {k:6d} {E/len(X_train):6.1%} {err_test:6.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adjusting the margin (b):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  alpha       b      K TrErr  TeErr\n",
      "------- ------- ------ ------ ------\n",
      "1.0e-01 0.0e+00     10   0.0%  25.0%\n",
      "1.0e-01 1.0e-02     10   2.5%  15.0%\n",
      "1.0e-01 1.0e-01     10   0.0%   5.0%\n",
      "1.0e-01 1.0e+00     10  28.7%   5.0%\n",
      "1.0e-01 1.0e+01     10  98.8%  10.0%\n",
      "1.0e-01 1.0e+02     10 100.0%  65.0%\n"
     ]
    }
   ],
   "source": [
    "print(f\"  alpha       b      K TrErr  TeErr\")\n",
    "print(f\"------- ------- ------ ------ ------\")\n",
    "a = 0.1; K = 10\n",
    "for b in (.0, .01, .1, 1, 10, 100):\n",
    "    W, E, k = PerceptronTraining(X_train, y_train, b=b, a=a, K=K)\n",
    "    y_test_pred = PerceptronClassification(X_test,W)\n",
    "    err_test = np.count_nonzero(y_test_pred != y_test) / len(X_test)\n",
    "    print(f\"{a:.1e} {b:.1e} {k:6d} {E/len(X_train):6.1%} {err_test:6.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final classifier:** $\\;$ Training final classifier with best parameters, saving and loading to test it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 0.1; a = 0.1; K = 10 # Replace with the best configuration obtained in the previous experiments\n",
    "W, E, k = PerceptronTraining(X, y, b=b, a=a, K=K)\n",
    "np.save(\"MyDigitsWeights.npy\",W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('MyDigitsWeights.npy', 'rb') as fd:\n",
    "    W = np.load(fd)\n",
    "y_test_pred = PerceptronClassification(X_test,W)\n",
    "err_test = np.count_nonzero(y_test_pred != y_test) / len(X_test)\n",
    "print(f\"Test error of final classifier: {err_test:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell only when running in Google Colab \n",
    "# You need to download MyDigitsWeights.npy\n",
    "#files.download('MyDigitsWeights.npy') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify your own handwritten digits\n",
    "\n",
    "<p style=\"text-align: justify\">The following simple application allows you to classify your own handwritten digits. When you run this application, it shows a basic graphical interface containing a panel on which you can draw your own handwritten digits.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">Before you can draw a digit, you need to click on the *pen* locate on the left vertical. Then you can draw on the panel. If you need to erase what you have drawn on the panel, just click on *bin* located on the top menu.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">You can classify the image on the panel by clicking on the bottom bar labeled with *Classify image\".</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell only when running in Google Colab \n",
    "# You need to upload DigitClassifyGradioApp.py\n",
    "#from google.colab import files\n",
    "#uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mDigitClassifyGradioApp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_interface\n\u001b[32m      3\u001b[39m fn = \u001b[38;5;28minput\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPlease provide filename for weight matrix:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fd:\n\u001b[32m      5\u001b[39m     W = np.load(fd)\n\u001b[32m      7\u001b[39m demo = create_interface(W, PerceptronClassification)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/University/SIN/Pract/venv/lib64/python3.13/site-packages/IPython/core/interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: ''"
     ]
    }
   ],
   "source": [
    "from DigitClassifyGradioApp import create_interface\n",
    "\n",
    "fn = input(\"Please provide filename for weight matrix:\")\n",
    "with open(fn, 'rb') as fd:\n",
    "    W = np.load(fd)\n",
    "\n",
    "demo = create_interface(W, PerceptronClassification)\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
